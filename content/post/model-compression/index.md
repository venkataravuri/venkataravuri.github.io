---
title: ML Model Compression - A practicle guide to Pruning, Quantization and Distillation
description: Reduce size of ML models without loosing their performance and efficiency and improving inferences speed usng techniques such as Pruning, Quantization and Distillation.
slug: model-compression
date: 2024-02-21 00:00:00+0000
image: cover.jpg
tags:
    - PyTorch
weight: 1 # You can add weight to some posts to override the default sorting (date descending)
---
Model Compression techniques makes models smaller and simpler without losing their effectiveness. Compression offer benefits such as,
1. Reduce the size of your model, which makes it easier to store, transfer, and deploy.
2. Smaller models also require less memory, which makes them perfect for resource-constrained devices. 
3. Improve inference speed, allowing for faster predictions and real-time applications.
4. Compressed models consume less energy, can be deployed to edge devices such as mobiles, glasses, etc.

## Pruning: Trimming the Excess

https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb

## Quantization: Shrinking the Precision

## Distillation: Knowledge Transfer for Efficiency

https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb

Distillation is a process of training a smaller, more compact model to mimic the behavior of a larger, more complex model. 

By transferring knowledge from the larger model, distillation enables the creation of highly efficient models without sacrificing performance. This technique has been particularly effective in scenarios where computational resources are limited, such as deploying models on edge devices, smartphones, tablets, etc. A large language model can be distilled into a smaller model that retains most of the original modelâ€™s performance while being more lightweight and faster to execute.
