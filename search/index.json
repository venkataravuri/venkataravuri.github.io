[{"content":"Deep learning models often have too many parameters, which require significant GPU resources and time for inference. Model compression techniques makes models smaller and simpler without losing their effectiveness by reducing number of parameters. Compression benefits,\nReduce the size of your model, which makes it easier to store, transfer, and deploy. Smaller models also require less memory, which makes them perfect for resource-constrained devices. Improve inference speed, allowing for faster predictions and real-time applications. Compressed models consume less energy, can be deployed to edge devices such as mobiles, glasses, etc. :scissors: Pruning: Trimming the Excess Model pruning enhances model efficiency by removing unnecessary components, such as weights or entire neurons, from a trained neural network. This process helps reduce the model\u0026rsquo;s size and improves its speed without significantly sacrificing accuracy.\nPruning can be broadly categorized into two types: unstructured pruning and structured pruning.\nUnstructured Pruning Unstructured pruning focuses on individual weights within the model. This method involves setting certain weights to zero based on specific criteria, such as their magnitude. By removing these less significant weights, the model remains compact while still retaining its overall functionality.\nStructured Pruning In contrast, structured pruning removes larger, predefined groups of parameters, such as entire neurons, filters, or layers. This approach takes into account the architecture of the network and aims to maintain its structural integrity while reducing complexity.\nLocal Pruning Local pruning refers to the practice of applying pruning techniques at the level of individual layers or components within the model. In this approach, each layer is analyzed independently, and pruning decisions are made based on the characteristics of that specific layer.\nGlobal Pruning In contrast, global pruning considers the entire model as a whole when making pruning decisions. This technique identifies and removes the least significant connections across all layers simultaneously. By doing so, global pruning often achieves better overall performance and efficiency compared to local methods, as it allows for more comprehensive optimization of the network\u0026rsquo;s structure.\nPruning Tutorial :white_large_square: :white_medium_square: Quantization: Shrinking the Precision Model quantization reduces the size and computational requirements of neural networks by converting high-precision weights and activations into lower-precision formats.\nDifferent numerical formats are used to represent weights and activations in neural networks.\nFloat32 (FP32): a 32-bit floating point, is the standard representation for real numbers in many machine learning models. Bfloat16 (BF16): a 16-bit floating-point format that retains the exponent size of float32 but reduces the number of bits used for the mantissa. This allows it to represent a similar range as float32 but with less precision. Int8: an 8-bit integer format that can represent values from [−128,127][−128,127]. Quantization techniques aim to convert high-precision representations (like float32) into lower-precision formats (like int8 or int4) to enhance performance and reduce resource consumption in deep learning models.\nQuantization Techniques Post-Training Quantization (PTQ): PTQ involves quantizing a pre-trained model after its training phase. Common approaches within PTQ include:\nParameter Quantization: Reduces the precision of weights and biases. Dynamic Range Quantization: Adjusts the precision of weights based on observed activation ranges. Mixed-Precision Quantization: Combines different precision levels for weights and activations to balance accuracy and efficiency. Quantization-Aware Training (QAT): In contrast to PTQ, QAT integrates quantization during the training process. Key aspects include:\nActivation Quantization: Involves quantizing not just weights but also intermediate activation values during inference. Fake Quantization: Simulates the effects of quantization during training, helping the model adjust accordingly. There two popular libraries for quantization,\nGPTQ (Generalized Post-Training Quantization): GPTQ employs a mixed quantization strategy using INT4 for weights while keeping activations in higher precision (FP16). Bitsandbytes: Bitsandbytes is a library designed for efficient training and inference of large language models. It provides tools for quantizing models, enabling reduced memory consumption and faster processing times. The library supports various quantization techniques, including both PTQ and QAT, making it versatile for different use cases. :woman_teacher: :man_student: Distillation: Knowledge Transfer for Efficiency Distillation is a process of training a smaller, more compact model to mimic the behavior of a larger, more complex model.\nBy transferring knowledge from the larger model, distillation enables the creation of highly efficient models without sacrificing performance. This technique has been particularly effective in scenarios where computational resources are limited, such as deploying models on edge devices, smartphones, tablets, etc. A large language model can be distilled into a smaller model that retains most of the original model’s performance while being more lightweight and faster to execute.\nKnowledge Distillation Tutorial ","date":"2024-02-21T00:00:00Z","image":"https://venkataravuri.github.io/cover.jpg","permalink":"https://venkataravuri.github.io/p/model-compression/","title":"ML Model Compression - A practicle guide to Pruning, Quantization and Distillation"},{"content":"In this article, we explore tecniques to improve training and inference performance of Deep Learning models built using PyTorch.\nPyTorch 2.0 introduces new compiler technologies to improve model performance and runtime efficiency and target diverse hardware backends by wrapping model with simple command torch.compile(). PyTorch 2.0 compiler translates high-level code written in deep learning frameworks into optimized lower level hardware specific code to accelerate training and inference. To improve performance, a deep learning compiler has to take advantage of hardware specific features such as mixed precision support, performance optimized kernels and minimize communication between host (CPU), Operator Fusion and CPU/GPU Code-Generation and AI accelerator.\nThere are different phases of the compilation process,\ntorch.compile makes PyTorch code run faster by JIT-compiling PyTorch code into optimized kernels. torch.compile significantly enhances model performance by optimizing both the computation graph and the execution of operations on hardware accelerators, leading to faster inference and training times.\nWhat happens when a model wrapped with torch.compile(model)? When torch.compile is invoked in PyTorch, it performs several background steps to optimize model execution by using several components. The torch.compile model goes through the following steps before execution,\nTorchDynamo is responsible for JIT compiling arbitrary Python code into FX graphs (a graph of tensor operations), which can then be further optimized. TorchDynamo extracts FX graphs by analyzing Python bytecode during runtime and detecting calls to PyTorch operations. If it comes across a segment of code that it cannot interpret, it defaults to the regular Python interpreter. This approach ensures that it can handle a wide range of programs while providing significant performance improvements.\nAOTAutograd Automatically generates the backward computation graph from the forward computation graph, ensuring that gradients can be computed efficiently. Its function is to produce backward traces in an ahead-of-time fashion, enhancing the efficiency of the differentiation process. This enables acceleration of both the forward and backward pass.\nTorchInductor The default backend that compiles the computation graph into optimized low-level code suitable for execution on various hardware accelerators. It takes the computation graph generated by TorchDynamo and converts it into optimized low-level kernels. For NVIDIA and AMD GPUs, it employs OpenAI Triton\nTriton is a new programming language that provides much higher productivity than CUDA, but with the ability to beat the performance of highly optimized libraries like cuDNN with clean and simple code. It is developed by Philippe Tillet at OpenAI, and is seeing enormous adoption and traction across the industry. Triton supports NVIDIA GPUs, . It is quickly growing in popularity as a replacement for hand written CUDA kernels. C++/OpenMP is a widely adopted specification for writing parallel kernels. OpenMP provides a work sharing parallel execution model, and enables support for CPUs. Let’s now demonstrate that using torch.compile can speed up real models. We will compare standard eager mode and torch.compile by evaluating and training a sample CNN model on CIFAR10 dataset.\nModel performance in Eager mode Model performance in torch.compile mode Layer Fusion Layer fusion aims to combine multiple layers of a neural network into a single layer, thereby reducing the computational overhead associated with separate operations.\nHow Layer Fusion Works\nSimilarity Assessment: The process begins by evaluating the similarity between layers using a distance metric, such as cosine similarity, based on their weights. Selection of Layers: The top-k similar layers are identified. Weight Freezing: For each selected layer, one set of weights is frozen while the other continues to be updated during training. This approach allows the model to maintain efficiency without significantly sacrificing accuracy Operator Fusion Operator fusion is another optimization technique that merges different operations into a single computational step.\nMechanism of Operator Fusion\nIn typical scenarios, an operation like convolution is followed by an activation function (e.g., ReLU). Without fusion, the results from the convolution must first be written to memory before the activation can be computed, leading to delays. By fusing these operations, the activation function can be executed immediately after the convolution without waiting for memory writes, thus enhancing performance template epilogue fusions, tiling, and horizontal/vertical fusions.\n","date":"2024-01-06T00:00:00Z","image":"https://venkataravuri.github.io/p/pytorch-optimization/cover_hu6307248181568134095.jpg","permalink":"https://venkataravuri.github.io/p/pytorch-optimization/","title":"Speed up PyTorch ML Models"}]