<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Reduce size of ML models without loosing their performance and efficiency and improving inferences speed usng techniques such as Pruning, Quantization and Distillation."><title>ML Model Compression - A practicle guide to Pruning, Quantization and Distillation</title>
<link rel=canonical href=https://venkataravuri.github.io/p/model-compression/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="ML Model Compression - A practicle guide to Pruning, Quantization and Distillation"><meta property='og:description' content="Reduce size of ML models without loosing their performance and efficiency and improving inferences speed usng techniques such as Pruning, Quantization and Distillation."><meta property='og:url' content='https://venkataravuri.github.io/p/model-compression/'><meta property='og:site_name' content='Venkata Ravuri'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='PyTorch'><meta property='article:published_time' content='2024-02-21T00:00:00+00:00'><meta property='article:modified_time' content='2024-02-21T00:00:00+00:00'><meta property='og:image' content='https://venkataravuri.github.io/cover.jpg'><meta name=twitter:title content="ML Model Compression - A practicle guide to Pruning, Quantization and Distillation"><meta name=twitter:description content="Reduce size of ML models without loosing their performance and efficiency and improving inferences speed usng techniques such as Pruning, Quantization and Distillation."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://venkataravuri.github.io/cover.jpg'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu897059592634026878.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>üç•</span></figure><div class=site-meta><h1 class=site-name><a href=/>Venkata Ravuri</a></h1><h2 class=site-description>Principal AI/ML Engineer</h2></div></header><ol class=menu-social><li><a href=https://github.com/venkataravuri target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://x.com/ravuri target=_blank title=X rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#scissors-pruning-trimming-the-excess>:scissors: Pruning: Trimming the Excess</a><ol><li><a href=#unstructured-pruning>Unstructured Pruning</a></li><li><a href=#structured-pruning>Structured Pruning</a></li><li><a href=#local-pruning>Local Pruning</a></li><li><a href=#global-pruning>Global Pruning</a></li></ol></li><li><a href=#white_large_square-white_medium_square-quantization-shrinking-the-precision>:white_large_square: :white_medium_square: Quantization: Shrinking the Precision</a><ol><li><a href=#quantization-techniques>Quantization Techniques</a></li></ol></li><li><a href=#woman_teacher-man_student-distillation-knowledge-transfer-for-efficiency>:woman_teacher: :man_student: Distillation: Knowledge Transfer for Efficiency</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/model-compression/><img src=/cover.jpg loading=lazy alt="Featured image of post ML Model Compression - A practicle guide to Pruning, Quantization and Distillation"></a></div><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/p/model-compression/>ML Model Compression - A practicle guide to Pruning, Quantization and Distillation</a></h2><h3 class=article-subtitle>Reduce size of ML models without loosing their performance and efficiency and improving inferences speed usng techniques such as Pruning, Quantization and Distillation.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Feb 21, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>4 minute read</time></div></footer></div></header><section class=article-content><p>Deep learning models often have too many parameters, which require significant GPU resources and time for inference. Model compression techniques makes models smaller and simpler without losing their effectiveness by reducing number of parameters. Compression benefits,</p><ol><li>Reduce the size of your model, which makes it easier to store, transfer, and deploy.</li><li>Smaller models also require less memory, which makes them perfect for resource-constrained devices.</li><li>Improve inference speed, allowing for faster predictions and real-time applications.</li><li>Compressed models consume less energy, can be deployed to edge devices such as mobiles, glasses, etc.</li></ol><h2 id=scissors-pruning-trimming-the-excess>:scissors: Pruning: Trimming the Excess</h2><p>Model pruning enhances model efficiency by removing unnecessary components, such as weights or entire neurons, from a trained neural network. This process helps reduce the model&rsquo;s size and improves its speed without significantly sacrificing accuracy.</p><p>Pruning can be broadly categorized into two types: <strong>unstructured pruning</strong> and <strong>structured pruning</strong>.</p><h3 id=unstructured-pruning>Unstructured Pruning</h3><p>Unstructured pruning focuses on individual weights within the model. This method involves setting certain weights to zero based on specific criteria, such as their magnitude. By removing these less significant weights, the model remains compact while still retaining its overall functionality.</p><h3 id=structured-pruning>Structured Pruning</h3><p>In contrast, structured pruning removes larger, predefined groups of parameters, such as entire neurons, filters, or layers. This approach takes into account the architecture of the network and aims to maintain its structural integrity while reducing complexity.</p><h3 id=local-pruning>Local Pruning</h3><p>Local pruning refers to the practice of applying pruning techniques at the level of individual layers or components within the model. In this approach, each layer is analyzed independently, and pruning decisions are made based on the characteristics of that specific layer.</p><h3 id=global-pruning>Global Pruning</h3><p>In contrast, global pruning considers the entire model as a whole when making pruning decisions. This technique identifies and removes the least significant connections across all layers simultaneously. By doing so, global pruning often achieves better overall performance and efficiency compared to local methods, as it allows for more comprehensive optimization of the network&rsquo;s structure.</p><p>Pruning Tutorial <a class=link href=https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb target=_blank rel=noopener><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p><h2 id=white_large_square-white_medium_square-quantization-shrinking-the-precision>:white_large_square: :white_medium_square: Quantization: Shrinking the Precision</h2><p>Model quantization reduces the size and computational requirements of neural networks by converting high-precision weights and activations into lower-precision formats.</p><p>Different numerical formats are used to represent weights and activations in neural networks.</p><ul><li>Float32 (FP32): a 32-bit floating point, is the standard representation for real numbers in many machine learning models.</li><li>Bfloat16 (BF16): a 16-bit floating-point format that retains the exponent size of float32 but reduces the number of bits used for the mantissa. This allows it to represent a similar range as float32 but with less precision.</li><li>Int8: an 8-bit integer format that can represent values from [‚àí128,127][‚àí128,127].</li><li></li></ul><p>Quantization techniques aim to convert high-precision representations (like float32) into lower-precision formats (like int8 or int4) to enhance performance and reduce resource consumption in deep learning models.</p><h3 id=quantization-techniques>Quantization Techniques</h3><ul><li><p><strong>Post-Training Quantization (PTQ)</strong>: PTQ involves quantizing a pre-trained model after its training phase. Common approaches within PTQ include:</p><ul><li>Parameter Quantization: Reduces the precision of weights and biases.</li><li>Dynamic Range Quantization: Adjusts the precision of weights based on observed activation ranges.</li><li>Mixed-Precision Quantization: Combines different precision levels for weights and activations to balance accuracy and efficiency.</li></ul></li><li><p><strong>Quantization-Aware Training (QAT)</strong>: In contrast to PTQ, QAT integrates quantization during the training process. Key aspects include:</p><ul><li>Activation Quantization: Involves quantizing not just weights but also intermediate activation values during inference.</li><li>Fake Quantization: Simulates the effects of quantization during training, helping the model adjust accordingly.</li></ul></li></ul><p>There two popular libraries for quantization,</p><ul><li><strong>GPTQ (Generalized Post-Training Quantization)</strong>: GPTQ employs a mixed quantization strategy using INT4 for weights while keeping activations in higher precision (FP16).</li><li><strong>Bitsandbytes</strong>: Bitsandbytes is a library designed for efficient training and inference of large language models. It provides tools for quantizing models, enabling reduced memory consumption and faster processing times. The library supports various quantization techniques, including both PTQ and QAT, making it versatile for different use cases.</li></ul><h2 id=woman_teacher-man_student-distillation-knowledge-transfer-for-efficiency>:woman_teacher: :man_student: Distillation: Knowledge Transfer for Efficiency</h2><p>Distillation is a process of training a smaller, more compact model to mimic the behavior of a larger, more complex model.</p><p>By transferring knowledge from the larger model, distillation enables the creation of highly efficient models without sacrificing performance. This technique has been particularly effective in scenarios where computational resources are limited, such as deploying models on edge devices, smartphones, tablets, etc. A large language model can be distilled into a smaller model that retains most of the original model‚Äôs performance while being more lightweight and faster to execute.</p><p>Knowledge Distillation Tutorial <a class=link href=https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/a19d8941b0ebb13c102e41c7e24bc5fb/knowledge_distillation_tutorial.ipynb target=_blank rel=noopener><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p></section><footer class=article-footer><section class=article-tags><a href=/tags/pytorch/>PyTorch</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Feb 21, 2024 00:00 UTC</span></section></footer></article><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//venkataravuri.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2024 Venkata Ravuri</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.28.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>